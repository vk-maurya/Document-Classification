{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CdZH9UHEqoA1"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/bazingagin/npc_gzip.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install npc-gzip datasets portalocker>=2.0.0 ipython-autotime scikit-learn -q"
      ],
      "metadata": {
        "id": "xp2iefQmq4Qq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "from torchtext.datasets import IMDB\n",
        "\n",
        "from npc_gzip.compressors.base import BaseCompressor\n",
        "from npc_gzip.compressors.gzip_compressor import GZipCompressor\n",
        "from npc_gzip.knn_classifier import KnnClassifier\n",
        "import pandas as pd\n",
        "from tqdm import tqdm, tqdm_pandas\n",
        "%load_ext autotime\n",
        "import random\n",
        "import numpy as np\n",
        "np.random.seed(seed=21)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzvvWhtDrERY",
        "outputId": "ee190a2c-6b5e-4be3-f682-1844830d4a61"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 441 µs (started: 2023-08-20 12:25:03 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk -q\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2fzZkEvuazq",
        "outputId": "e7e03e57-a640-45f7-96c6-de3042f76af4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 8.09 s (started: 2023-08-20 12:25:03 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Initialize stopwords and lemmatizer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "    cleaned_text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    cleaned_text = cleaned_text.lower()\n",
        "    cleaned_text = ' '.join(cleaned_text.split())\n",
        "    words = cleaned_text.split()\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    return ' '.join(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AN3H4KBuUns",
        "outputId": "305ad0c6-c8e1-4e6b-b72c-0ec1572a1d43"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 5.83 ms (started: 2023-08-20 12:25:11 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "input_text = \"This is an example text with some extra    spaces and irrelevant words.\"\n",
        "cleaned_output = clean_text(input_text)\n",
        "print(cleaned_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6UPKyGYzvxw",
        "outputId": "3fcb5787-8678-4cf4-e443-d9807702a46c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "example text extra space irrelevant word\n",
            "time: 2.37 s (started: 2023-08-20 12:25:11 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "data = load_dataset('imdb')\n",
        "data['train'] = data['train'].shuffle(seed=42)\n",
        "data['test'] = data['test'].shuffle(seed=42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LakzFMaXu4Jq",
        "outputId": "26a96f7c-ec31-44fc-b04c-7c1c6187cf95"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.04 s (started: 2023-08-20 12:25:14 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# conveting to pandas\n",
        "train = data['train'].to_pandas()\n",
        "test  = data['test'].to_pandas()\n",
        "print(train.columns)\n",
        "## cleaning text\n",
        "train['clean_text'] = train['text'].apply(lambda x:clean_text(x))\n",
        "test['clean_text'] = test['text'].apply(lambda x:clean_text(x))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSCMUY0KvYMh",
        "outputId": "d03674d9-f995-45dc-dc77-58ade32c2a24"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['text', 'label'], dtype='object')\n",
            "time: 37 s (started: 2023-08-20 12:27:07 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NPC_GZIP Model"
      ],
      "metadata": {
        "id": "Yfb-ZIbQ2JGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#converting to numpy\n",
        "train_cl_text,train_cl_labels = train['clean_text'].values,train['label'].values\n",
        "test_cl_text,test_cl_labels = test['clean_text'].values,test['label'].values\n",
        "print(f\"Training dataset size {train_cl_text.shape}\")\n",
        "print(f\"Testin dataset size {test_cl_text.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JjCG65q2Hgu",
        "outputId": "b00939b0-2a0c-4e05-eaa2-80dca4f5ab8a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset size (25000,)\n",
            "Testin dataset size (25000,)\n",
            "time: 3.18 ms (started: 2023-08-20 12:27:44 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_model(\n",
        "    train_text: np.ndarray, train_labels: np.ndarray, distance_metric: str = \"ncd\"\n",
        ") -> KnnClassifier:\n",
        "    compressor: BaseCompressor = GZipCompressor()\n",
        "    model: KnnClassifier = KnnClassifier(\n",
        "        compressor=compressor,\n",
        "        training_inputs=train_text,\n",
        "        training_labels=train_labels,\n",
        "        distance_metric=distance_metric,\n",
        "    )\n",
        "    return model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYikoJOHrQlD",
        "outputId": "69e5824d-7e76-49e6-a073-61cf44d82f06"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 803 µs (started: 2023-08-20 12:27:44 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main(train_text, train_labels,test_text,test_labels) -> None:\n",
        "    print(\"Fitting model...\")\n",
        "    model = fit_model(train_text, train_labels)\n",
        "    print(\"Generating predictions...\")\n",
        "    top_k = 1\n",
        "    (distances, labels, similar_samples) = model.predict(\n",
        "        test_text, top_k, sampling_percentage=0.01\n",
        "    )\n",
        "    print(classification_report(test_labels, labels.reshape(-1)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izpbjvolrWsx",
        "outputId": "fa44818d-1547-41e6-ca44-3a23fd6f5712"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 913 µs (started: 2023-08-20 12:28:09 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model on text data"
      ],
      "metadata": {
        "id": "zvxMf4JN3D44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#converting to numpy\n",
        "train_text,train_labels = train['text'].values,train['label'].values\n",
        "test_text,test_labels = test['text'].values,test['label'].values\n",
        "print(f\"Training dataset size {train_text.shape}\")\n",
        "print(f\"Testin dataset size {test_text.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpdhQuA13Vx9",
        "outputId": "df58c599-7897-4553-eaed-7772e81fb619"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset size (25000,)\n",
            "Testin dataset size (25000,)\n",
            "time: 3.79 ms (started: 2023-08-20 12:28:18 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main(train_text, train_labels,test_text,test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtdPHNDu3JnW",
        "outputId": "27ec1e44-47ee-45a7-e35a-2a42e1d98bc1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting model...\n",
            "Generating predictions...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Compressing input...: 100%|██████████| 25000/25000 [13:39<00:00, 30.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.65      0.60     12500\n",
            "           1       0.59      0.49      0.54     12500\n",
            "\n",
            "    accuracy                           0.57     25000\n",
            "   macro avg       0.57      0.57      0.57     25000\n",
            "weighted avg       0.57      0.57      0.57     25000\n",
            "\n",
            "time: 13min 46s (started: 2023-08-20 12:28:24 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model on clean text data"
      ],
      "metadata": {
        "id": "nkI7MQQu2-hU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#converting to numpy\n",
        "train_cl_text,train_cl_labels = train['clean_text'].values,train['label'].values\n",
        "test_cl_text,test_cl_labels = test['clean_text'].values,test['label'].values\n",
        "print(f\"Training dataset size {train_cl_text.shape}\")\n",
        "print(f\"Testin dataset size {test_cl_text.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TrkRkj53Twb",
        "outputId": "faf3e8ee-8e19-4d91-c3bb-89f66d3d157f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset size (25000,)\n",
            "Testin dataset size (25000,)\n",
            "time: 3.73 ms (started: 2023-08-20 12:45:33 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main(train_cl_text, train_cl_labels,test_cl_text,test_cl_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlW0yEk2rZ9K",
        "outputId": "0d4c0138-75a4-4d71-cde9-2062013c5337"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting model...\n",
            "Generating predictions...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Compressing input...: 100%|██████████| 25000/25000 [08:51<00:00, 47.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.55      0.57     12500\n",
            "           1       0.57      0.61      0.59     12500\n",
            "\n",
            "    accuracy                           0.58     25000\n",
            "   macro avg       0.58      0.58      0.58     25000\n",
            "weighted avg       0.58      0.58      0.58     25000\n",
            "\n",
            "time: 8min 59s (started: 2023-08-20 12:45:36 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest with TFIDF"
      ],
      "metadata": {
        "id": "IXzs9FBY3B_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "def train_and_evaluate_random_forest(train_text, train_labels, test_text, test_labels):\n",
        "    # Initialize TF-IDF vectorizer\n",
        "    tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "    # Transform the training text data to TF-IDF features\n",
        "    train_tfidf = tfidf_vectorizer.fit_transform(train_text)\n",
        "\n",
        "    # Transform the test text data using the same TF-IDF vectorizer\n",
        "    test_tfidf = tfidf_vectorizer.transform(test_text)\n",
        "\n",
        "    # Initialize the Random Forest classifier\n",
        "    rf_classifier = RandomForestClassifier(n_estimators=100,n_jobs=-1,random_state=42)\n",
        "\n",
        "    # Train the model\n",
        "    rf_classifier.fit(train_tfidf, train_labels)\n",
        "\n",
        "    # Make predictions on the test data\n",
        "    predictions = rf_classifier.predict(test_tfidf)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(test_labels, predictions)\n",
        "\n",
        "    # Print the classification report\n",
        "    class_report = classification_report(test_labels, predictions)\n",
        "    print(class_report)\n",
        "\n",
        "\n",
        "train_and_evaluate_random_forest(train_cl_text, train_cl_labels,test_cl_text,test_cl_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBkG_qVp4TDG",
        "outputId": "e28344b1-24e0-4938-8709-a853d9d04994"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.86      0.84     12500\n",
            "           1       0.85      0.82      0.84     12500\n",
            "\n",
            "    accuracy                           0.84     25000\n",
            "   macro avg       0.84      0.84      0.84     25000\n",
            "weighted avg       0.84      0.84      0.84     25000\n",
            "\n",
            "time: 54.4 s (started: 2023-08-20 13:00:35 +00:00)\n"
          ]
        }
      ]
    }
  ]
}